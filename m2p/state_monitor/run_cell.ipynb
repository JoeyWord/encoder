
#%%
from sklearn.datasets import load_iris



#%%
iris=load_iris()


#%%
from sklearn.model_selection import train_test_split


#%%
x_train,x_dev,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.4,random_state=0)


#%%
x_train.shape


#%%
import lightgbm as lgb


#%%
train_data=lgb.Dataset(x_train,label=y_train)


#%%
dev_data=lgb.Dataset(x_dev,label=y_test)


#%%
params = {
    'task': 'train',
    'boosting_type': 'gbdt',  # 设置提升类型
    'objective': 'regression', # 目标函数
    'metric': {'l2', 'auc'},  # 评估函数
    'num_leaves': 31,   # 叶子节点数
    'learning_rate': 0.05,  # 学习速率
    'feature_fraction': 0.9, # 建树的特征选择比例
    'bagging_fraction': 0.8, # 建树的样本采样比例
    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging
    'verbose': 1 # <0 显示致命的, =0 显示错误 (警告), >0 显示信息
}


#%%
lgb.cv(params,train_data)


#%%
cv_res=lgb.cv(params,train_data)


#%%
cv_res.get("auc_mean")


#%%
cv_res.get("auc-mean")


#%%
len(cv_res.get("auc-mean"))


#%%
cv_res.get("auc-mean").index(1)


#%%
bst=lgb.train(params,train_data,valid_sets=[dev_data],early_stopping_rounds=10)


#%%
bst.best_iteration


#%%
x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.4,random_state=0)


#%%
x_input,x_dev,y_input,y_dev=train_test_split(x_train,y_train,test_size=0.1,random_state=0)


#%%
x_dev.shape


#%%
cv_train=lgb.Dataset(x_train,label=y_train)


#%%
cv_res=lgb.cv(params,cv_train)


#%%
cv_res.get("auc-mean").index(1)


#%%
train_data=lgb.Dataset(x_input,label=y_input)
dev_data=lgb.Dataset(x_dev,label=y_dev)
bst=lgb.train(params,train_data,valid_sets=[dev_data],early_stop_rounds=10)


#%%
train_data=lgb.Dataset(x_input,label=y_input)
dev_data=lgb.Dataset(x_dev,label=y_dev)
bst=lgb.train(params,train_data,valid_sets=[dev_data],early_stopping_rounds=10)


#%%
best_iter=bst.best_iteration
bst.save_model("iris_model.txt",num_iteration=best_iter)


#%%
pred=bst.predict(x_test,num_iteration=best_iter)


#%%
y_test.shape


#%%
pred


#%%
pred.shape


#%%
params = {
    "objective":"multiclass",
    "num_class":3,
    "metics":"auc",
    "learning_rate":0.05,
}


#%%
params = {
    "objective":"multiclass",
    "num_class":3,
    "metics":"multi_logloss",
    "learning_rate":0.05,
}


#%%
new_cv=lgb.cv(params,cv_train)


#%%
params


#%%
params = {
    "objective":"multiclass",
    "num_class":3,
    "metric":"multi_logloss",
    "learning_rate":0.05,
}


#%%
new_cv=lgb.cv(params,cv_train)


#%%
nround=new_cv.get("multi_logloss-mean").index(min(new_cv.get("multi_logloss-mean")))


#%%
nround


#%%
bst=lgb.train(params,cv_train,num_boost_round=nround)


#%%
bst.best_iteration


#%%
bst.save_model("iris_model_init.txt",num_iteration=nround)


#%%
y_hat=bst.predict(x_test,num_iteration=99)
print(y_hat.shape)
from numpy import argmax
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)


#%%
accuracy


#%%
load_bst=lgb.Booster(params=params,model_file="iris_model.txt")


#%%
y_hat=load_bst.predict(x_test,num_iteration=99)
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)
print("accuracy= %.2f%%" %(accuracy*100))


#%%
y_hat=load_bst.predict(x_test,num_iteration=99)
print(y_hat.shape)
from numpy import argmax
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)
print("accuracy= %.2f%%" %(accuracy*100))


#%%
load_bst.params


#%%
load_bst.update(train_set=cv_train)


#%%
y_hat=load_bst.predict(x_test,num_iteration=99)
print(y_hat.shape)
from numpy import argmax
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)
print("accuracy= %.2f%%" %(accuracy*100))


#%%
from sklearn.externals import joblib


#%%
joblib.dump(bst,"booster.pkl")


#%%
bst_job=joblib.load("booster.pkl")


#%%
y_hat=bst_job.predict(x_test,num_iteration=99)
print(y_hat.shape)
from numpy import argmax
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)
print("accuracy= %.2f%%" %(accuracy*100))


#%%
load_bst.current_iteration


#%%
load_bst.update(train_set=cv_train)


#%%
load_bst.update(train_set=cv_train)


#%%
y_hat=load_bst.predict(x_test,num_iteration=99)
print(y_hat.shape)
from numpy import argmax
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)
print("accuracy= %.2f%%" %(accuracy*100))


#%%
bst_init=lgb.Booster(model_file="iris_model.txt")


#%%
bst_init.params


#%%
y_hat=bst_init.predict(x_test,num_iteration=99)
print(y_hat.shape)
from numpy import argmax
pred = [argmax(_p) for _p in y_hat]
print("test samples length={}".format(len(pred)))
accuracy=sum([1 if _p==_r else 0 for _p,_r in zip(pred,y_test)])/len(pred)
print("accuracy= %.2f%%" %(accuracy*100))


#%%
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
'''
@File    :   df_demo.py
@Time    :   2019/06/28 23:29:25
@Author  :   Jin Weihua 
@Version :   1.0
@Contact :   jwhV587@gmail.com
@License :   (C)Copyright 2017-2018, Liugroup-NLPR-CASIA
@Desc    :   None
'''

# here put the import lib
import pandas as pd
from numpy import arange,random
from copy import copy,deepcopy

_data = arange(0,9)
_data = _data.reshape(3,3)
_col = ["a","b","c"]
_df = pd.DataFrame(_data,columns=_col)
print("now df\n",_df)
_df_cpy = copy(_df)
print("_df id=",id(_df))
print("_df_cpy id=",id(_df_cpy))
random.shuffle(_df.values)
print("current df\n",_df)


#%%
print(_df_cpy)


#%%
_df_cpy.columns


#%%
_df["d"]=[None,0,0]


#%%
def fill_na(row):
    if row["d"]:
        row["d"] += 1
    else:
        row["d"] = 0
    return row


#%%
get_ipython().run_line_magic('pinfo', 'fill_na')


#%%
_df.apply(fill_na,axis=1)


#%%
_df


#%%
def fill_na(row):
    
    row["d"] += 1
    return row


#%%
_df.apply(fill_na,axis=1)


#%%
def fill_na(row):
    if not row[0]:
        row[0] = 0
    return row


#%%
_df.apply(fill_na,axis=1)


#%%
_df.apply(fill_na,axis=0)


#%%
def add_col(row):
    row["e"] = row["a"] + row["b"]
    return row


#%%
_df.apply(fill_na,axis=1)


#%%
def add_col(row):
    row["e"] = row["a"] + row["b"]
    


#%%
add_func=lambda x: x[0]+x[1]


#%%
_df.apply(add_func,axis=1)


#%%
add_func=lambda x: x["a"]+x["b"]


#%%
_df.apply(add_func,axis=1)


#%%
_df


#%%
def add_col(row):
    row["e"] = row[0] + row[1]
    return row
    


#%%
_df.apply(add_func,axis=1)


#%%
_df.apply(add_col,axis=1)


#%%
def add_col(row):
    row["e"] = row["a"] + row["b"]
    return row
    


#%%
_df.apply(add_col,axis=1)


#%%
fill_na


#%%
def fill_na(row):
    if row["d"]==0:
        row["d"] += 1
    else:
        row["d"]=0
    return row


#%%
_df.apply(fill_na,axis=1)


#%%
from numpy import NaN


#%%
def fill_na(row):
    if row["d"] == NaN or row["d"] is None:
        row["d"] = 0
    else:
        row["d"]=1
    return row


#%%
_df.apply(fill_na,axis=1)


#%%
_df


#%%
_df.d[0]


#%%
if _df.d[0] is NaN:
    print(1)


#%%
type(_df.d[0])


#%%
type(NaN)


#%%
type(_df.d[1])


#%%
if not _df.d[0]:
    print(1)


#%%
if not _df.d[1]:
    print(1)


#%%
if _df.d[0]:
    print(1)


#%%
if _df.d[0] is None:
    print(1)
    


#%%
sum(_df["d"].values)


#%%
for v in _df["d"]:
    if v is None:
        print("y")


#%%
for v in _df["d"].values:
    if v is None:
        print("y")


#%%
_df["d"].values


#%%
from numpy import isnan


#%%
for v in _df["d"].values:
    if isnan(v):
        print("y")


#%%
from numpy import inf


#%%
if isnan(inf):
    print(1)


#%%
_df_add=_df.apply(add_col,axis=1)


#%%
_df_add


#%%
from numpy import cast


#%%
def fill_na(row):
    if isnan(row["d"]):
        row["d"] = 0
    else:
        row["d"]=1
    return row


#%%
_df.apply(fill_na,axis=1)


#%%
_df["e"]=['1','2','ww']


#%%
from pandas import to_numeric


#%%
to_numeric(_df["e"],errors="ignore")


#%%
to_numeric(_df["e"],errors="cores")


#%%
_df.a.astype(int)


#%%
_df.a.astype(int,inplace=True)


#%%
_df


#%%
_df_add.a.astype(int,inplace=True)


#%%
_df_add


#%%
_df_add["a"]=_df_add.a.astype(int,inplace=True)


#%%
_df_add


#%%
_df.a


#%%
_df_add.a


#%%
from sklearn.datasets import load_iris


#%%
iris=load_iris()


#%%
iris.keys


#%%
iris.data


#%%
iris.feature_names


#%%
iris.target


#%%
iris.target_names


#%%
from sklearn.model_selection import train_test_split


#%%
x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.4,random_state=0)


